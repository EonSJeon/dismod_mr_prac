{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 'prevalence' sheet to 'prevalence.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the 'prevalence' sheet using openpyxl\n",
    "df = pd.read_excel(\n",
    "    'full_data.xlsx',\n",
    "    sheet_name='Prevalence',\n",
    "    engine='openpyxl'\n",
    ")\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('prevalence.csv', index=False)\n",
    "\n",
    "print(\"Saved 'prevalence' sheet to 'prevalence.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done — all PMIDs filled and saved to 'prevalence_filled.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Load the sheet (use openpyxl for .xlsx)\n",
    "df = pd.read_csv('prevalence.csv')\n",
    "\n",
    "# 2. Forward-fill PMID so every row has the correct value\n",
    "df['PMID'] = df['PMID'].ffill()\n",
    "\n",
    "# 3. Save to CSV\n",
    "df.to_csv('prevalence_filled.csv', index=False)\n",
    "\n",
    "print(\"Done — all PMIDs filled and saved to 'prevalence_filled.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved with columns: index, sex, area, effective_sample_size, data_type, type, value, standard_error, age_start, age_end, year_start, year_end.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Load and reset index\n",
    "df = pd.read_csv('prevalence_filled.csv').reset_index(drop=True)\n",
    "\n",
    "# 2. Drop rows missing any of the critical fields\n",
    "df = df.dropna(subset=[\n",
    "    'age_start', 'age_end', 'year_start', 'year_end',\n",
    "    'Sample size', 'proportion',\n",
    "    'Type (Any, early to intermediate, late-wet, late-dry)'\n",
    "])\n",
    "\n",
    "# 3. Cast age and year columns to integers\n",
    "for col in ['age_start', 'age_end', 'year_start', 'year_end']:\n",
    "    df[col] = df[col].astype(int)\n",
    "\n",
    "# 4. Build 'area' from Nation (if non-NA) otherwise Region\n",
    "df['area'] = df['Nation'].fillna(df['Region'])\n",
    "\n",
    "# 5. Set constant data_type = 'p'\n",
    "df['data_type'] = 'p'\n",
    "\n",
    "# 6. Lowercase sex\n",
    "df['sex'] = df['Sex'].str.lower()\n",
    "\n",
    "# 7. Reset index into its own column, then select & reorder (including Type)\n",
    "out = df.reset_index()[[\n",
    "    'index',\n",
    "    'sex',\n",
    "    'area',\n",
    "    'Sample size',\n",
    "    'data_type',\n",
    "    'Type (Any, early to intermediate, late-wet, late-dry)',\n",
    "    'proportion',\n",
    "    'standard error',\n",
    "    'age_start',\n",
    "    'age_end',\n",
    "    'year_start',\n",
    "    'year_end'\n",
    "]].rename(columns={\n",
    "    'Sample size': 'effective_sample_size',\n",
    "    'proportion': 'value',\n",
    "    'standard error': 'standard_error',\n",
    "    'Type (Any, early to intermediate, late-wet, late-dry)': 'type'\n",
    "})\n",
    "\n",
    "# 8. Save final CSV\n",
    "out.to_csv('prevalence_filled_selected.csv', index=False)\n",
    "\n",
    "print(\"Saved with columns: index, sex, area, effective_sample_size, data_type, type, value, standard_error, age_start, age_end, year_start, year_end.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Late                     1555\n",
      "Early to intermediate     978\n",
      "Any                       939\n",
      "Late-dry                  712\n",
      "Late-wet                  707\n",
      "Intermediate              612\n",
      "Early                     254\n",
      "Name: type, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Load\n",
    "df = pd.read_csv('prevalence_filled_selected.csv')\n",
    "\n",
    "# 2. Normalize: strip spaces, lower-case\n",
    "df['type_norm'] = df['type'].str.strip().str.lower()\n",
    "\n",
    "# 3. Map to canonical labels\n",
    "type_map = {\n",
    "    'late':                   'Late',\n",
    "    'early to intermediate':  'Early to intermediate',\n",
    "    'any':                    'Any',\n",
    "    'late-dry':               'Late-dry',\n",
    "    'late-wet':               'Late-wet',\n",
    "    'intermediate':           'Intermediate',\n",
    "    'early':                  'Early'\n",
    "}\n",
    "df['type'] = df['type_norm'].map(type_map)\n",
    "\n",
    "# 4. (Optional) drop the helper column\n",
    "df = df.drop(columns='type_norm')\n",
    "\n",
    "# 5. Check\n",
    "print(df['type'].value_counts())\n",
    "\n",
    "# 6. Save\n",
    "df.to_csv('prevalence_filled_selected.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns now present: ['index', 'sex', 'area', 'effective_sample_size', 'data_type', 'value', 'standard_error', 'age_start', 'age_end', 'year_start', 'year_end']\n",
      "\n",
      "Saved filtered data without 'type' column to 'prevalence_late_categories.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Load the selected CSV\n",
    "df = pd.read_csv('prevalence_filled_selected.csv')\n",
    "\n",
    "# 2. Filter to only Late, Late-dry, Late-wet\n",
    "keep_types = ['Late', 'Late-dry', 'Late-wet']\n",
    "df_filtered = df[df['type'].isin(keep_types)].reset_index(drop=True)\n",
    "\n",
    "# 3. Drop the 'type' column entirely\n",
    "df_filtered = df_filtered.drop(columns='type')\n",
    "\n",
    "# 4. Optional: confirm the column has been removed\n",
    "print(\"Columns now present:\", df_filtered.columns.tolist())\n",
    "\n",
    "# 5. Save to a new CSV\n",
    "df_filtered.to_csv('prevalence_late_categories.csv', index=False)\n",
    "\n",
    "print(\"\\nSaved filtered data without 'type' column to 'prevalence_late_categories.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['US' 'Austrailia' 'Netherland' 'Finland' 'Barbados' 'Italy' 'Greece'\n",
      " 'Japan' 'France' 'Iceland' 'India' 'China' '7 countries' 'Norway'\n",
      " 'Estonia' 'Northern Ireland' 'Spain' 'Greenland' 'Brazil' 'Taiwan'\n",
      " 'Singapore' 'Thailand' 'UK' 'Germany' 'Kenya' 'Netherlands' 'South Korea'\n",
      " 'Ireland' 'Algeria' 'Portugal' 'Slovakia' 'Russia' 'Iran']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV\n",
    "df = pd.read_csv('prevalence_late_categories.csv')\n",
    "\n",
    "# Print unique area values\n",
    "print(df['area'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 90 rows with unmapped area.\n",
      "Unmapped area values were: ['7 countries']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Load the filtered CSV\n",
    "df = pd.read_csv('prevalence_late_categories.csv')\n",
    "\n",
    "# 2. Keep a copy of the original area strings\n",
    "df['orig_area'] = df['area']\n",
    "\n",
    "# 3. Define ISO3 mapping (Greenland → Denmark)\n",
    "area_to_iso = {\n",
    "    'US':               'USA',\n",
    "    'Austrailia':       'AUS',\n",
    "    'Netherland':       'NLD',\n",
    "    'Netherlands':      'NLD',\n",
    "    'Finland':          'FIN',\n",
    "    'Barbados':         'BRB',\n",
    "    'Italy':            'ITA',\n",
    "    'Greece':           'GRC',\n",
    "    'Japan':            'JPN',\n",
    "    'France':           'FRA',\n",
    "    'Iceland':          'ISL',\n",
    "    'India':            'IND',\n",
    "    'China':            'CHN',\n",
    "    'Norway':           'NOR',\n",
    "    'Estonia':          'EST',\n",
    "    'Northern Ireland': 'GBR',\n",
    "    'Spain':            'ESP',\n",
    "    'Greenland':        'DNK',   # Denmark\n",
    "    'Brazil':           'BRA',\n",
    "    'Taiwan':           'TWN',\n",
    "    'Singapore':        'SGP',\n",
    "    'Thailand':         'THA',\n",
    "    'UK':               'GBR',\n",
    "    'United Kingdom':   'GBR',\n",
    "    'South Korea':      'KOR',\n",
    "    'Germany':          'DEU',\n",
    "    'Kenya':            'KEN',\n",
    "    'Ireland':          'IRL',\n",
    "    'Algeria':          'DZA',\n",
    "    'Portugal':         'PRT',\n",
    "    'Slovakia':         'SVK',\n",
    "    'Russia':           'RUS',\n",
    "    'Iran':             'IRN'\n",
    "}\n",
    "\n",
    "# 4. Map area → ISO3\n",
    "df['area'] = df['area'].map(area_to_iso)\n",
    "\n",
    "# 5. Identify unmapped rows\n",
    "mask_unmapped = df['area'].isna()\n",
    "num_dropped = mask_unmapped.sum()\n",
    "unmapped_vals = df.loc[mask_unmapped, 'orig_area'].unique().tolist()\n",
    "\n",
    "# 6. Report which area strings were dropped\n",
    "print(f\"Dropped {num_dropped} rows with unmapped area.\")\n",
    "print(\"Unmapped area values were:\", unmapped_vals)\n",
    "\n",
    "# 7. Drop them and reset index\n",
    "df = df[~mask_unmapped].reset_index(drop=True)\n",
    "\n",
    "# 8. (Optional) remove helper column\n",
    "df = df.drop(columns=['orig_area'])\n",
    "\n",
    "# 9. Save updated CSV\n",
    "df.to_csv('prevalence_late_categories_country_code.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted 588 rows where 'value' was zero.\n",
      "Removed 0 rows where 'value' exceeded 1.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load the CSV\n",
    "df.to_csv('prevalence_late_categories_country_code.csv', index=False)\n",
    "\n",
    "# 2. Find rows where value == 0 and fix\n",
    "mask_zero = df['value'] == 0\n",
    "df.loc[mask_zero, 'value'] = 1 / (2 * df.loc[mask_zero, 'effective_sample_size'])\n",
    "df.loc[mask_zero, 'standard_error'] = np.sqrt(\n",
    "    df.loc[mask_zero, 'value'] * (1 - df.loc[mask_zero, 'value']) / df.loc[mask_zero, 'effective_sample_size']\n",
    ")\n",
    "\n",
    "# 3. Cast effective_sample_size to integer\n",
    "df['effective_sample_size'] = df['effective_sample_size'].astype(int)\n",
    "\n",
    "# 4. Drop rows where value > 1\n",
    "mask_gt1 = df['value'] > 1\n",
    "num_gt1 = mask_gt1.sum()\n",
    "df = df[~mask_gt1]\n",
    "\n",
    "# 5. Report counts\n",
    "print(f\"Adjusted {mask_zero.sum()} rows where 'value' was zero.\")\n",
    "print(f\"Removed {num_gt1} rows where 'value' exceeded 1.\")\n",
    "\n",
    "# 6. Save back to CSV\n",
    "df.to_csv('input_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read output template and drop covariates\n",
    "template = pd.read_csv('output_template.csv')\n",
    "template = template.drop(columns=['x_cv_ascertainment', 'x_cv_diagnostic_criteria', \n",
    "                                'x_cv_representative', 'x_ihme_fao_stimulants_kcal_26oct11',\n",
    "                                'x_smoking_prev'])\n",
    "template.to_csv('output_template.csv', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 186 unique areas:\n",
      "['AFG', 'AGO', 'ALB', 'ARE', 'ARG', 'ARM', 'ATG', 'AUS', 'AUT', 'AZE', 'BDI', 'BEL', 'BEN', 'BFA', 'BGD', 'BGR', 'BHR', 'BHS', 'BIH', 'BLR', 'BLZ', 'BOL', 'BRA', 'BRB', 'BRN', 'BTN', 'BWA', 'CAF', 'CAN', 'CHE', 'CHL', 'CHN', 'CIV', 'CMR', 'COD', 'COG', 'COL', 'COM', 'CPV', 'CRI', 'CUB', 'CYP', 'CZE', 'DEU', 'DJI', 'DMA', 'DNK', 'DOM', 'DZA', 'ECU', 'EGY', 'ERI', 'ESP', 'EST', 'ETH', 'FIN', 'FJI', 'FRA', 'FSM', 'GAB', 'GBR', 'GEO', 'GHA', 'GIN', 'GMB', 'GNB', 'GNQ', 'GRC', 'GRD', 'GTM', 'GUY', 'HND', 'HRV', 'HTI', 'HUN', 'IDN', 'IND', 'IRL', 'IRN', 'IRQ', 'ISL', 'ISR', 'ITA', 'JAM', 'JOR', 'JPN', 'KAZ', 'KEN', 'KGZ', 'KHM', 'KIR', 'KOR', 'KWT', 'LAO', 'LBN', 'LBR', 'LBY', 'LCA', 'LKA', 'LSO', 'LTU', 'LUX', 'LVA', 'MAR', 'MDA', 'MDG', 'MDV', 'MEX', 'MHL', 'MKD', 'MLI', 'MLT', 'MMR', 'MNE', 'MNG', 'MOZ', 'MRT', 'MUS', 'MWI', 'MYS', 'NAM', 'NER', 'NGA', 'NIC', 'NLD', 'NOR', 'NPL', 'NZL', 'OMN', 'PAK', 'PAN', 'PER', 'PHL', 'PNG', 'POL', 'PRK', 'PRT', 'PRY', 'PSE', 'QAT', 'ROU', 'RUS', 'RWA', 'SAU', 'SDN', 'SEN', 'SGP', 'SLB', 'SLE', 'SLV', 'SOM', 'SRB', 'STP', 'SUR', 'SVK', 'SVN', 'SWE', 'SWZ', 'SYC', 'SYR', 'TCD', 'TGO', 'THA', 'TJK', 'TKM', 'TLS', 'TON', 'TTO', 'TUN', 'TUR', 'TWN', 'TZA', 'UGA', 'UKR', 'URY', 'USA', 'UZB', 'VCT', 'VEN', 'VNM', 'VUT', 'WSM', 'YEM', 'ZAF', 'ZMB', 'ZWE']\n"
     ]
    }
   ],
   "source": [
    "# Read output template and extract unique area keys\n",
    "areas = pd.read_csv('output_template.csv')['area'].unique()\n",
    "print(f\"Found {len(areas)} unique areas:\")\n",
    "print(sorted(areas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 30 unique areas in input data:\n",
      "['USA' 'AUS' 'NLD' 'FIN' 'BRB' 'ITA' 'GRC' 'JPN' 'FRA' 'ISL' 'IND' 'CHN'\n",
      " 'NOR' 'EST' 'GBR' 'ESP' 'DNK' 'BRA' 'TWN' 'SGP' 'THA' 'DEU' 'KEN' 'KOR'\n",
      " 'IRL' 'DZA' 'PRT' 'SVK' 'RUS' 'IRN']\n"
     ]
    }
   ],
   "source": [
    "# Read input data and extract unique area keys\n",
    "input_areas = pd.read_csv('input_data.csv')['area'].unique()\n",
    "print(f\"\\nFound {len(input_areas)} unique areas in input data:\")\n",
    "print(input_areas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 189 valid area codes in hierarchy:\n",
      "['AFG', 'AGO', 'ALB', 'ARE', 'ARG', 'ARM', 'ATG', 'AUS', 'AUT', 'AZE', 'BDI', 'BEL', 'BEN', 'BFA', 'BGD', 'BGR', 'BHR', 'BHS', 'BIH', 'BLR', 'BLZ', 'BOL', 'BRA', 'BRB', 'BRN', 'BTN', 'BWA', 'CAF', 'CAN', 'CHE', 'CHL', 'CHN', 'CIV', 'CMR', 'COD', 'COG', 'COL', 'COM', 'CPV', 'CRI', 'CUB', 'CYP', 'CZE', 'DEU', 'DJI', 'DNK', 'DOM', 'DZA', 'ECU', 'EGY', 'ERI', 'ESP', 'EST', 'ETH', 'FIN', 'FJI', 'FRA', 'FSM', 'GAB', 'GBR', 'GEO', 'GHA', 'GIN', 'GMB', 'GNB', 'GNQ', 'GRC', 'GRD', 'GTM', 'GUM', 'GUY', 'HND', 'HRV', 'HTI', 'HUN', 'IDN', 'IND', 'IRL', 'IRN', 'IRQ', 'ISL', 'ISR', 'ITA', 'JAM', 'JOR', 'JPN', 'KAZ', 'KEN', 'KGZ', 'KHM', 'KIR', 'KOR', 'KWT', 'LAO', 'LBN', 'LBR', 'LBY', 'LCA', 'LKA', 'LSO', 'LTU', 'LUX', 'LVA', 'MAR', 'MDA', 'MDG', 'MDV', 'MEX', 'MHL', 'MKD', 'MLI', 'MLT', 'MMR', 'MNE', 'MNG', 'MOZ', 'MRT', 'MUS', 'MWI', 'MYS', 'NAM', 'NER', 'NGA', 'NIC', 'NLD', 'NOR', 'NPL', 'NZL', 'OMN', 'PAK', 'PAN', 'PER', 'PHL', 'PNG', 'POL', 'PRI', 'PRK', 'PRT', 'PRY', 'PSE', 'QAT', 'ROU', 'RUS', 'RWA', 'SAU', 'SDN', 'SEN', 'SGP', 'SLB', 'SLE', 'SLV', 'SOM', 'SRB', 'SSD', 'STP', 'SUR', 'SVK', 'SVN', 'SWE', 'SWZ', 'SYC', 'SYR', 'TCD', 'TGO', 'THA', 'TJK', 'TKM', 'TLS', 'TON', 'TTO', 'TUN', 'TUR', 'TWN', 'TZA', 'UGA', 'UKR', 'URY', 'USA', 'UZB', 'VCT', 'VEN', 'VIR', 'VNM', 'VUT', 'WSM', 'YEM', 'ZAF', 'ZMB', 'ZWE']\n",
      "\n",
      "Areas in hierarchy but not in template:\n",
      "['GUM', 'PRI', 'SSD', 'VIR']\n",
      "\n",
      "Areas in template but not in hierarchy:\n",
      "['DMA']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def load_jsonc(path):\n",
    "    with open(path, 'r') as f:\n",
    "        text = f.read()\n",
    "    # 1) strip single-line \"//…\" comments\n",
    "    text = re.sub(r'//.*?(?=\\n)', '', text)\n",
    "    # 2) strip block \"/* … */\" comments\n",
    "    text = re.sub(r'/\\*.*?\\*/', '', text, flags=re.S)\n",
    "    # 3) remove trailing commas before } or ]\n",
    "    text = re.sub(r',\\s*([\\}\\]])', r'\\1', text)\n",
    "    return json.loads(text)\n",
    "\n",
    "hierarchy = load_jsonc('hierarchy.jsonc')\n",
    "all_nodes = [node[0] for node in hierarchy['nodes']]\n",
    "\n",
    "# Only keep valid 3-letter country codes\n",
    "node_names = [area for area in all_nodes if len(area) == 3 and area.isupper()]\n",
    "\n",
    "print(f\"Found {len(node_names)} valid area codes in hierarchy:\")\n",
    "print(sorted(node_names))\n",
    "\n",
    "# Find areas in node_names but not in areas\n",
    "in_nodes_not_areas = set(node_names) - set(areas)\n",
    "print(\"\\nAreas in hierarchy but not in template:\")\n",
    "print(sorted(in_nodes_not_areas))\n",
    "\n",
    "# Find areas in areas but not in node_names \n",
    "in_areas_not_nodes = set(areas) - set(node_names)\n",
    "print(\"\\nAreas in template but not in hierarchy:\")\n",
    "print(sorted(in_areas_not_nodes))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation errors found:\n",
      " - Node 'ATG' (level 3) has no parent\n",
      " - Node 'BHS' (level 3) has no parent\n",
      " - Node 'BRB' (level 3) has no parent\n",
      " - Node 'BLZ' (level 3) has no parent\n",
      " - Node 'CUB' (level 3) has no parent\n",
      " - Node 'DOM' (level 3) has no parent\n",
      " - Node 'GRD' (level 3) has no parent\n",
      " - Node 'GUY' (level 3) has no parent\n",
      " - Node 'HTI' (level 3) has no parent\n",
      " - Node 'JAM' (level 3) has no parent\n",
      " - Node 'PRI' (level 3) has no parent\n",
      " - Node 'LCA' (level 3) has no parent\n",
      " - Node 'VCT' (level 3) has no parent\n",
      " - Node 'SUR' (level 3) has no parent\n",
      " - Node 'TTO' (level 3) has no parent\n",
      " - Node 'VIR' (level 3) has no parent\n",
      " - Node 'BOL' (level 3) has no parent\n",
      " - Node 'ECU' (level 3) has no parent\n",
      " - Node 'PER' (level 3) has no parent\n",
      " - Node 'COL' (level 3) has no parent\n",
      " - Node 'CRI' (level 3) has no parent\n",
      " - Node 'SLV' (level 3) has no parent\n",
      " - Node 'GTM' (level 3) has no parent\n",
      " - Node 'HND' (level 3) has no parent\n",
      " - Node 'MEX' (level 3) has no parent\n",
      " - Node 'NIC' (level 3) has no parent\n",
      " - Node 'PAN' (level 3) has no parent\n",
      " - Node 'VEN' (level 3) has no parent\n",
      " - Node 'BRA' (level 3) has no parent\n",
      " - Node 'PRY' (level 3) has no parent\n",
      " - Node 'SDN' has multiple parents: ['sub-saharan_africa_east', 'north_africa_middle_east']\n",
      " - Node 'SSD' (level 3) has no parent\n",
      " - Unreachable nodes from root 'all': {'MEX', 'PAN', 'LCA', 'BRA', 'VEN', 'BHS', 'SUR', 'PER', 'VCT', 'SLV', 'VIR', 'JAM', 'ECU', 'CUB', 'BLZ', 'GRD', 'CRI', 'GTM', 'ATG', 'DOM', 'PRI', 'HTI', 'BRB', 'BOL', 'TTO', 'HND', 'NIC', 'COL', 'SSD', 'PRY', 'GUY'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "def validate_graph_tree(graph):\n",
    "    # Build level map and edge list\n",
    "    level_map = {name: info[\"level\"] for name, info in graph[\"nodes\"]}\n",
    "    edges = [(src, dst) for src, dst, _ in graph[\"edges\"]]\n",
    "    errors = []\n",
    "\n",
    "    # Identify root (level 0)\n",
    "    roots = [n for n, lvl in level_map.items() if lvl == 0]\n",
    "    if len(roots) != 1:\n",
    "        errors.append(f\"Expected exactly one root, found: {roots}\")\n",
    "        root = roots[0] if roots else None\n",
    "    else:\n",
    "        root = roots[0]\n",
    "\n",
    "    # Build parent mapping\n",
    "    parents = defaultdict(list)\n",
    "    for src, dst in edges:\n",
    "        parents[dst].append(src)\n",
    "\n",
    "    # Check parent counts and root parent\n",
    "    for node, lvl in level_map.items():\n",
    "        if node == root:\n",
    "            if parents.get(node):\n",
    "                errors.append(f\"Root node '{root}' should have no parent, has: {parents[node]}\")\n",
    "        else:\n",
    "            if node not in parents:\n",
    "                errors.append(f\"Node '{node}' (level {lvl}) has no parent\")\n",
    "            elif len(parents[node]) > 1:\n",
    "                errors.append(f\"Node '{node}' has multiple parents: {parents[node]}\")\n",
    "\n",
    "    # Check level consistency on edges\n",
    "    for src, dst in edges:\n",
    "        if src not in level_map or dst not in level_map:\n",
    "            errors.append(f\"Edge {src}->{dst} references unknown node(s)\")\n",
    "            continue\n",
    "        if level_map[dst] != level_map[src] + 1:\n",
    "            errors.append(f\"Edge {src}->{dst} has invalid level jump: {level_map[src]} -> {level_map[dst]}\")\n",
    "\n",
    "    # Check connectivity and absence of cycles\n",
    "    visited = set()\n",
    "    rec_stack = set()\n",
    "\n",
    "    def dfs(node):\n",
    "        if node in rec_stack:\n",
    "            errors.append(f\"Cycle detected at node '{node}'\")\n",
    "            return\n",
    "        rec_stack.add(node)\n",
    "        visited.add(node)\n",
    "        for s, d in edges:\n",
    "            if s == node:\n",
    "                dfs(d)\n",
    "        rec_stack.remove(node)\n",
    "\n",
    "    if root:\n",
    "        dfs(root)\n",
    "        missing = set(level_map) - visited\n",
    "        if missing:\n",
    "            errors.append(f\"Unreachable nodes from root '{root}': {missing}\")\n",
    "\n",
    "    return errors\n",
    "\n",
    "def load_jsonc(path):\n",
    "    with open(path, 'r') as f:\n",
    "        text = f.read()\n",
    "    # 1) strip single-line \"//…\" comments\n",
    "    text = re.sub(r'//.*?(?=\\n)', '', text)\n",
    "    # 2) strip block \"/* … */\" comments\n",
    "    text = re.sub(r'/\\*.*?\\*/', '', text, flags=re.S)\n",
    "    # 3) remove trailing commas before } or ]\n",
    "    text = re.sub(r',\\s*([\\}\\]])', r'\\1', text)\n",
    "    return json.loads(text)\n",
    "\n",
    "graph = load_jsonc('hierarchy.jsonc')\n",
    "\n",
    "\n",
    "errs = validate_graph_tree(graph)\n",
    "if not errs:\n",
    "    print(\"Graph is a valid tree structure from root -> levels 1 -> 2 -> 3.\")\n",
    "else:\n",
    "    print(\"Validation errors found:\")\n",
    "    for err in errs:\n",
    "        print(\" -\", err)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dismod_mr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
